{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca7133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Set, List, Dict, Any, Tuple\n",
    "\n",
    "# Import your cleaning functions (ensure these are accessible)\n",
    "from survey_assist_utils.data_cleaning.prep_data import (\n",
    "    prep_clerical_codes,\n",
    "    prep_model_codes,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347e37a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. Logic Engine: \"Main\" (Set Theory / OO, OM, MO, MM)\n",
    "# ----------------------------------------------------------------------\n",
    "def evaluate_main_method(cc_set: Set[str], llm_set: Set[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluates matches based on Set Theory (The 'Main' Method).\n",
    "    Returns the classification (OO, OM, etc) and the boolean match result.\n",
    "    \"\"\"\n",
    "    if not cc_set or not llm_set:\n",
    "        return {\"main_case\": \"Missing\", \"main_result\": False}\n",
    "\n",
    "    cc_len = len(cc_set)\n",
    "    llm_len = len(llm_set)\n",
    "    \n",
    "   \n",
    "    # 'One to One' (OO)\n",
    "    if cc_len == 1 and llm_len == 1:\n",
    "        return {\n",
    "            \"main_case\": \"One-to-One\", \n",
    "            \"main_result\": cc_set == llm_set\n",
    "        }\n",
    "\n",
    "    # 'One to Many' (OM) - CC is single, LLM is multiple\n",
    "    # Does the CC label appear in the LLM list?\n",
    "    if cc_len == 1 and llm_len > 1:\n",
    "        return {\n",
    "            \"main_case\": \"One-to-Many\",\n",
    "            \"main_result\": cc_set.issubset(llm_set)\n",
    "        }\n",
    "\n",
    "    # 'Many to One' (MO) - CC is multiple, LLM is single\n",
    "    # Is the LLM label in the CC list?\n",
    "    if cc_len > 1 and llm_len == 1:\n",
    "        return {\n",
    "            \"main_case\": \"Many-to-One\",\n",
    "            \"main_result\": llm_set.issubset(cc_set)\n",
    "        }\n",
    "\n",
    "    # 'Many to Many' (MM)\n",
    "    # Do their lists have at least one item in common?\n",
    "    if cc_len > 1 and llm_len > 1:\n",
    "        return {\n",
    "            \"main_case\": \"Many-to-Many\",\n",
    "            \"main_result\": not cc_set.isdisjoint(llm_set)\n",
    "        }\n",
    "        \n",
    "    return {\"main_case\": \"Unhandled\", \"main_result\": False}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7fa4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Logic Engine: \"Legacy / 160c\" (Column / Positional)\n",
    "# ----------------------------------------------------------------------\n",
    "def evaluate_legacy_method(\n",
    "    cc_set: Set[str], \n",
    "    llm_set: Set[str], \n",
    "    config: str = \"col1_vs_col1\"\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Evaluates matches based on Column/Positional logic (The '160c' Method).\n",
    "    \n",
    "    Since sets are unordered, we must sort them to simulate 'Columns'.\n",
    "    Assumption: The 'Primary' code is the first item in a sorted list \n",
    "    (or however your data ingestion enforces order).\n",
    "    \"\"\"\n",
    "    # Convert sets to sorted lists to simulate \"Column 1, Column 2...\"\n",
    "    cc_list = sorted(list(cc_set))\n",
    "    llm_list = sorted(list(llm_set))\n",
    "    \n",
    "    # Helper to safe-get index\n",
    "    def get_val(lst, idx):\n",
    "        return lst[idx] if idx < len(lst) else None\n",
    "\n",
    "    # Logic Implementation based on config choices\n",
    "    if config == \"col1_vs_col1\":\n",
    "        # Check col 1 CC vs col1 LLM (Unambiguous flag logic usually implies strictness)\n",
    "        c1 = get_val(cc_list, 0)\n",
    "        l1 = get_val(llm_list, 0)\n",
    "        return c1 == l1 and c1 is not None\n",
    "\n",
    "    elif config == \"all_cc_vs_col1_llm\":\n",
    "        # Check if Col 1 LLM exists anywhere in CC columns\n",
    "        l1 = get_val(llm_list, 0)\n",
    "        if l1 is None: return False\n",
    "        return l1 in cc_list\n",
    "\n",
    "    elif config == \"col1_cc_vs_all_llm\":\n",
    "        # Check if Col 1 CC exists anywhere in LLM columns\n",
    "        c1 = get_val(cc_list, 0)\n",
    "        if c1 is None: return False\n",
    "        return c1 in llm_list\n",
    "        \n",
    "    elif config == \"any_vs_any\":\n",
    "        # Intersection check (similar to MM but usually strictly boolean without case logic)\n",
    "        return bool(set(cc_list) & set(llm_list))\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7110ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Comparison\n",
    "# ----------------------------------------------------------------------\n",
    "def run_comparison_pipeline(\n",
    "    cc_filepath: str, \n",
    "    llm_filepath: str,\n",
    "    legacy_mode: str = \"col1_vs_col1\"\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    # Load Data\n",
    "    print(\"Loading Data...\")\n",
    "    cc_df = pd.read_csv(cc_filepath)\n",
    "    llm_df = pd.read_parquet(llm_filepath) if llm_filepath.endswith('.parquet') else pd.read_csv(llm_filepath)\n",
    "\n",
    "    # Prep Data \n",
    "    print(\"Cleaning Data...\")\n",
    "    # Adjust args \n",
    "    cc_clean = prep_clerical_codes(cc_df) \n",
    "    llm_clean = prep_model_codes(llm_df) \n",
    "\n",
    "    # Merge\n",
    "    # We only care about unique_id and the cleaned sets\n",
    "    merged = pd.merge(\n",
    "        cc_clean[[\"unique_id\", \"clerical_codes\"]],\n",
    "        llm_clean[[\"unique_id\", \"model_codes\"]],\n",
    "        on=\"unique_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "\n",
    "    print(\"Running Evaluations...\")\n",
    "    for _, row in merged.iterrows():\n",
    "        uid = row['unique_id']\n",
    "        cc = row['clerical_codes']\n",
    "        llm = row['model_codes']\n",
    "\n",
    "        # Run Main\n",
    "        main_res = evaluate_main_method(cc, llm)\n",
    "\n",
    "        # Run Legacy\n",
    "        legacy_res = evaluate_legacy_method(cc, llm, config=legacy_mode)\n",
    "\n",
    "        # Record\n",
    "        results.append({\n",
    "            \"unique_id\": uid,\n",
    "            \"cc_codes\": str(cc),\n",
    "            \"llm_codes\": str(llm),\n",
    "            \"main_case\": main_res['main_case'],\n",
    "            \"main_result\": main_res['main_result'],\n",
    "            \"legacy_result\": legacy_res,\n",
    "            \"legacy_config\": legacy_mode,\n",
    "            # Do they agree?\n",
    "            \"methods_agree\": main_res['main_result'] == legacy_res\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5962e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example paths - update these\n",
    "CC_FILE = \"fake_cc_data.csv\"\n",
    "LLM_FILE = \"fake_llm_data.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13215e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Item '00000' has no valid codes.\n",
      "Item '00000' has no valid codes.\n",
      "Item '99999' has no valid codes.\n",
      "Item '99999' has no valid codes.\n",
      "Item '99999' has no valid codes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Data...\n",
      "Running Evaluations...\n"
     ]
    }
   ],
   "source": [
    "# Run comparison\n",
    "df_comparison = run_comparison_pipeline(CC_FILE, LLM_FILE, legacy_mode=\"col1_vs_col1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1e88ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Records: 7\n",
      "Disagreements: 0\n",
      "            unique_id            cc_codes  llm_codes    main_case  \\\n",
      "0            OO_Match           {'86101'}  {'86101'}   One-to-One   \n",
      "1         OO_Mismatch           {'86101'}      set()      Missing   \n",
      "2            OM_Match           {'86101'}  {'86101'}   One-to-One   \n",
      "3            MO_Match  {'72200', '86101'}  {'72200'}  Many-to-One   \n",
      "4            MM_Match  {'72200', '86101'}  {'72200'}  Many-to-One   \n",
      "5       Invalid_Input               set()      set()      Missing   \n",
      "6  Legacy_Unambiguous           {'01110'}  {'01110'}   One-to-One   \n",
      "\n",
      "   main_result  legacy_result legacy_config  methods_agree  \n",
      "0         True           True  col1_vs_col1           True  \n",
      "1        False          False  col1_vs_col1           True  \n",
      "2         True           True  col1_vs_col1           True  \n",
      "3         True           True  col1_vs_col1           True  \n",
      "4         True           True  col1_vs_col1           True  \n",
      "5        False          False  col1_vs_col1           True  \n",
      "6         True           True  col1_vs_col1           True  \n"
     ]
    }
   ],
   "source": [
    "# Analysis 1: Where do they disagree?\n",
    "disagreements = df_comparison[~df_comparison['methods_agree']]\n",
    "\n",
    "print(f\"\\nTotal Records: {len(df_comparison)}\")\n",
    "print(f\"Disagreements: {len(disagreements)}\")\n",
    "\n",
    "if not disagreements.empty:\n",
    "    print(\"\\nSample Disagreements:\")\n",
    "    print(disagreements[['unique_id', 'main_case', 'main_result', 'legacy_result', 'cc_codes', 'llm_codes']].head(10))\n",
    "\n",
    "    # Analysis 2: Disagreement by Case Type\n",
    "    print(\"\\nDisagreements by Main Case Type:\")\n",
    "    print(disagreements['main_case'].value_counts())\n",
    "\n",
    "print(df_comparison.head(10))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survey-assist-utils-Fq1cEvYD-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
